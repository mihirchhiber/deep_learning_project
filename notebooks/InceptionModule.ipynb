{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "Bu5u_7cwST8Z",
        "outputId": "720c074f-0447-4243-a018-d05567cbe254"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting opencv-python-headless<4.3\n",
            "  Downloading opencv_python_headless-4.2.0.34-cp37-cp37m-manylinux1_x86_64.whl (21.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 21.6 MB 1.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python-headless<4.3) (1.21.5)\n",
            "Installing collected packages: opencv-python-headless\n",
            "Successfully installed opencv-python-headless-4.2.0.34\n"
          ]
        }
      ],
      "source": [
        " !pip install \"opencv-python-headless<4.3\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2022-04-01T20:02:23.754496Z",
          "iopub.status.busy": "2022-04-01T20:02:23.753540Z",
          "iopub.status.idle": "2022-04-01T20:02:23.796312Z",
          "shell.execute_reply": "2022-04-01T20:02:23.795646Z",
          "shell.execute_reply.started": "2022-04-01T20:02:23.754444Z"
        },
        "id": "ZuFMtIHUST8z"
      },
      "outputs": [],
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.optim import lr_scheduler\n",
        "from torchvision import datasets, models, transforms\n",
        "import time\n",
        "import copy\n",
        "import cv2\n",
        "from sklearn.metrics import roc_curve, auc, roc_auc_score, classification_report, confusion_matrix\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "IlSUi5AUSpMW",
        "outputId": "3961e073-b9b0-463f-cc6b-2e6a4a7a663a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "G-urg4TjST86"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/Data/features_30_sec.csv\")\n",
        "df = df[['filename','label']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "X2l0hezbST88"
      },
      "outputs": [],
      "source": [
        "df = df[df['filename'] != \"jazz.00054.wav\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "HX3dvFj0ST9A"
      },
      "outputs": [],
      "source": [
        "df = df.reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "win-voUyST9D",
        "outputId": "8bf77b79-e63e-4200-fc67-7814a8995608"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0        0\n",
              "1        1\n",
              "2        2\n",
              "3        3\n",
              "4        4\n",
              "      ... \n",
              "994    995\n",
              "995    996\n",
              "996    997\n",
              "997    998\n",
              "998    999\n",
              "Name: index, Length: 999, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "df.pop('index')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "UmWAVs6BST9G"
      },
      "outputs": [],
      "source": [
        "class_name = {}\n",
        "n = 0\n",
        "for i in set(df['label']):\n",
        "    class_name[i] = n\n",
        "    n+=1\n",
        "num_classes = n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "5OV_pjXGST9M",
        "outputId": "95a2ac4b-a85f-4330-dba0-321b2c4020ac"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'blues': 3,\n",
              " 'classical': 9,\n",
              " 'country': 8,\n",
              " 'disco': 5,\n",
              " 'hiphop': 1,\n",
              " 'jazz': 0,\n",
              " 'metal': 7,\n",
              " 'pop': 4,\n",
              " 'reggae': 6,\n",
              " 'rock': 2}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "class_name"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "j0vDqY1LST9R"
      },
      "outputs": [],
      "source": [
        "df['label'] = df['label'].map(class_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "vnzHFHbSST9X",
        "outputId": "854b6a40-7adb-48c9-b385-c0119c66c024"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n"
          ]
        }
      ],
      "source": [
        "for i in range(len(df)):\n",
        "    temp = df['filename'][i].split(\".\")\n",
        "    df['filename'][i] = \"Data/images_original/\" + temp[0] + \"/\" + temp[0] + temp[1] + \".png\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "r92A4K3JST9b"
      },
      "outputs": [],
      "source": [
        "train, test = train_test_split(df, test_size=0.30, random_state=42, stratify = df['label'])\n",
        "test, val = train_test_split(test, test_size=0.50, random_state=42, stratify = test['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "AyPjuWR8ST9e"
      },
      "outputs": [],
      "source": [
        "dataset_sizes = {'train': len(train), 'test': len(test), 'val': len(val)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "iosynH4iST9f"
      },
      "outputs": [],
      "source": [
        "class GenreDataset(Dataset):\n",
        "    \"\"\"Genre dataset.\"\"\"\n",
        "\n",
        "    def __init__(self, csv_file, transform=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            csv_file (string): Path to the csv file with annotations.\n",
        "            transform (callable, optional): Optional transform to be applied\n",
        "                on a sample.\n",
        "        \"\"\"\n",
        "        self.csv = csv_file\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.csv)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        img_name = self.csv.iloc[idx, 0]\n",
        "        image = cv2.imread(\"/content/drive/MyDrive/\" + img_name,cv2.IMREAD_COLOR)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "        image = np.expand_dims(image, axis=-1)\n",
        "        details = self.csv.iloc[idx, 1:]\n",
        "        sample = {'image': image, 'label': details[0]}\n",
        "\n",
        "        if self.transform:\n",
        "            sample = self.transform(sample)\n",
        "\n",
        "        return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "BApuhqYsST9j"
      },
      "outputs": [],
      "source": [
        "class PreProcessing(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def __call__(self, sample):\n",
        "        image, turtle_id = sample['image'], sample['label']\n",
        "#         h, w = image.shape[:2]\n",
        "        \n",
        "        ### ADD PREPROCESSING CODE HERE\n",
        "#         image = np.array([image])\n",
        "        \n",
        "        return [np.transpose(image, (2, 1, 0)), turtle_id]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "FoEwqqAqST9m"
      },
      "outputs": [],
      "source": [
        "train_transformed_dataset = GenreDataset(csv_file=train,\n",
        "                                               transform=transforms.Compose([\n",
        "                                               PreProcessing()\n",
        "                                           ]))\n",
        "test_transformed_dataset = GenreDataset(csv_file=test,\n",
        "                                               transform=transforms.Compose([\n",
        "                                               PreProcessing()\n",
        "                                           ]))\n",
        "val_transformed_dataset = GenreDataset(csv_file=val,\n",
        "                                               transform=transforms.Compose([\n",
        "                                               PreProcessing()\n",
        "                                           ]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "HIeV3BtZST9p"
      },
      "outputs": [],
      "source": [
        "dataloaders = {'train' : DataLoader(train_transformed_dataset, batch_size=8,\n",
        "                        shuffle=True, num_workers=0),\n",
        "              'test' : DataLoader(test_transformed_dataset, batch_size=4,\n",
        "                        shuffle=True, num_workers=0),\n",
        "              'val' : DataLoader(val_transformed_dataset, batch_size=4,\n",
        "                        shuffle=True, num_workers=0)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "GLsB5DCpST9s"
      },
      "outputs": [],
      "source": [
        "\n",
        "from torchvision.utils import make_grid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "DbRUB1-2ST9u"
      },
      "outputs": [],
      "source": [
        "def show_images(images, nmax=64):\n",
        "    fig, ax = plt.subplots(figsize=(8, 8))\n",
        "    ax.set_xticks([]); ax.set_yticks([])\n",
        "    ax.imshow(make_grid((images[0].detach()[:nmax]), nrow=8).permute(1, 2, 0))\n",
        "def show_batch(dl, nmax=64):\n",
        "    for images in dl:\n",
        "        print(images[1].shape)\n",
        "        show_images(images, nmax)\n",
        "        break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "uwsp_NaxST9v"
      },
      "outputs": [],
      "source": [
        "def show_pics(image):\n",
        "    print(image.shape)\n",
        "    plt.imshow(image)\n",
        "    plt.pause(0.001)  # pause a bit so that plots are updated\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "zBR6J73zST90",
        "outputId": "64ebfc5b-7171-4731-f80b-87ab82caf682"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "XZWS_VPlST92"
      },
      "outputs": [],
      "source": [
        "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
        "        print('-' * 10)\n",
        "\n",
        "        # Each epoch has a training and validation phase\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                model.train()  # Set model to training mode\n",
        "            else:\n",
        "                model.eval()   # Set model to evaluate mode\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0\n",
        "\n",
        "            # Iterate over data.\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "#                 inputs = inputs.type(torch.DoubleTensor)\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # forward\n",
        "                # track history if only in train\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "                    outputs = model(inputs.float())\n",
        "                    _, preds = torch.max(outputs, 1)\n",
        "                    loss = criterion(outputs, labels)\n",
        "\n",
        "                    # backward + optimize only if in training phase\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                # statistics\n",
        "                running_loss += loss.item() * inputs.size(0)\n",
        "                running_corrects += torch.sum(preds == labels.data)\n",
        "            if phase == 'train':\n",
        "                scheduler.step()\n",
        "\n",
        "            epoch_loss = running_loss / dataset_sizes[phase]\n",
        "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
        "\n",
        "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
        "                phase, epoch_loss, epoch_acc))\n",
        "\n",
        "            # deep copy the model\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()\n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
        "        time_elapsed // 60, time_elapsed % 60))\n",
        "    print('Best val Acc: {:4f}'.format(best_acc))\n",
        "\n",
        "    # load best model weights\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rh0hZ4SuST96"
      },
      "source": [
        "### Need to define our model below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "q99duHdLST-C"
      },
      "outputs": [],
      "source": [
        "# Only one Inception module \n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def conv(ni, nf, ks=3, stride=1, pad=1, bias=False):\n",
        "    return nn.Conv2d(ni, nf, kernel_size=ks, stride=stride, padding=pad, bias=bias)\n",
        "\n",
        "def conv_layer(ni, nf, ks=3, stride=1, pad=1,act=True):\n",
        "    bn = nn.BatchNorm2d(nf)\n",
        "    layers = [conv(ni, nf, ks, stride, pad), bn]\n",
        "    act_fn = nn.ReLU(inplace=True)\n",
        "    if act: layers.append(act_fn)\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "class InceptionModule(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.branch1x1 = conv_layer(256*2, 64*2, ks=1, stride=1, pad=0)\n",
        "        \n",
        "        self.branch1x1_pool = conv_layer(256*2, 32*2, ks=1, stride=1, pad=0)\n",
        "        self.branch1x1_final = conv_layer(32*2, 32*2, ks=1, stride=1, pad=0)\n",
        "        \n",
        "        self.branch3x3_init = conv_layer(256*2, 128*2, ks=3, stride=1, pad=1)\n",
        "        self.branch3x3_final = conv_layer(128*2, 128*2, ks=3, stride=1, pad=1)\n",
        "        \n",
        "        self.branch5x5_init = conv_layer(256*2, 32*2, ks=5, stride=1, pad=2)\n",
        "        self.branch5x5_final = conv_layer(32*2, 32*2, ks=5, stride=1, pad=2)\n",
        "        \n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        x1 = self.branch1x1(x)\n",
        "        \n",
        "        x2 = self.branch1x1_pool(x)\n",
        "        x3 = self.branch1x1_final(x2)\n",
        "        \n",
        "        x4 = self.branch3x3_init(x)\n",
        "        x5 = self.branch3x3_final(x4)\n",
        "        \n",
        "        x6 = self.branch5x5_init(x)\n",
        "        x7 = self.branch5x5_final(x6)\n",
        "        \n",
        "        ans = [x1,x3,x5,x7]\n",
        "        return self.relu(torch.cat(ans,1))\n",
        "\n",
        "def conv_layer_averpl(ni, nf):\n",
        "    aver_pl = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "    return nn.Sequential(conv_layer(ni, nf), aver_pl)\n",
        "\n",
        "model_ft = nn.Sequential(\n",
        "    conv_layer_averpl(1, 128),\n",
        "    conv_layer_averpl(128, 256),\n",
        "    conv_layer_averpl(256, 512),\n",
        "    InceptionModule(),\n",
        "    nn.AdaptiveAvgPool2d((2,2)),\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(2048, 40),\n",
        "    nn.Linear(40, 10)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3 Inception modules with skip connections\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def conv(ni, nf, ks=3, stride=1, pad=1, bias=False):\n",
        "    return nn.Conv2d(ni, nf, kernel_size=ks, stride=stride, padding=pad, bias=bias)\n",
        "\n",
        "def conv_layer(ni, nf, ks=3, stride=1, pad=1,act=True):\n",
        "    bn = nn.BatchNorm2d(nf)\n",
        "    layers = [conv(ni, nf, ks, stride, pad), bn]\n",
        "    act_fn = nn.ReLU(inplace=True)\n",
        "    if act: layers.append(act_fn)\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "class InceptionModule(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.branch1x1 = conv_layer(256, 64, ks=1, stride=1, pad=0)\n",
        "        \n",
        "        self.branch1x1_pool = conv_layer(256, 32, ks=1, stride=1, pad=0)\n",
        "        self.branch1x1_final = conv_layer(32, 32, ks=1, stride=1, pad=0)\n",
        "        \n",
        "        self.branch3x3_init = conv_layer(256, 128, ks=3, stride=1, pad=1)\n",
        "        self.branch3x3_final = conv_layer(128, 128, ks=3, stride=1, pad=1)\n",
        "        \n",
        "        self.branch5x5_init = conv_layer(256, 32, ks=5, stride=1, pad=2)\n",
        "        self.branch5x5_final = conv_layer(32, 32, ks=5, stride=1, pad=2)\n",
        "        \n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        x1 = self.branch1x1(x)\n",
        "        \n",
        "        x2 = self.branch1x1_pool(x)\n",
        "        x3 = self.branch1x1_final(x2)\n",
        "        \n",
        "        x4 = self.branch3x3_init(x)\n",
        "        x5 = self.branch3x3_final(x4)\n",
        "        \n",
        "        x6 = self.branch5x5_init(x)\n",
        "        x7 = self.branch5x5_final(x6)\n",
        "        \n",
        "        ans = [x1,x3,x5,x7]\n",
        "        return self.relu(torch.cat(ans,1)) + x\n",
        "\n",
        "def conv_layer_averpl(ni, nf):\n",
        "    aver_pl = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "    return nn.Sequential(conv_layer(ni, nf), aver_pl)\n",
        "\n",
        "model_ft = nn.Sequential(\n",
        "    conv_layer_averpl(1, 128),\n",
        "    conv_layer_averpl(128, 256),\n",
        "    InceptionModule(),\n",
        "    InceptionModule(),\n",
        "    InceptionModule(),\n",
        "    nn.AdaptiveAvgPool2d((2,2)),\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(1024, 40),\n",
        "    nn.Linear(40, 10)\n",
        ")"
      ],
      "metadata": {
        "id": "8ma3s1kyY-56"
      },
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3 Inception module\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def conv(ni, nf, ks=3, stride=1, pad=1, bias=False):\n",
        "    return nn.Conv2d(ni, nf, kernel_size=ks, stride=stride, padding=pad, bias=bias)\n",
        "\n",
        "def conv_layer(ni, nf, ks=3, stride=1, pad=1,act=True):\n",
        "    bn = nn.BatchNorm2d(nf)\n",
        "    layers = [conv(ni, nf, ks, stride, pad), bn]\n",
        "    act_fn = nn.ReLU(inplace=True)\n",
        "    if act: layers.append(act_fn)\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "class InceptionModule_1(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.branch1x1 = conv_layer(1, 32, ks=1, stride=1, pad=0)\n",
        "        \n",
        "        self.branch1x1_pool = conv_layer(1, 16, ks=1, stride=1, pad=0)\n",
        "        self.branch1x1_final = conv_layer(16, 16, ks=1, stride=1, pad=0)\n",
        "        \n",
        "        self.branch3x3_init = conv_layer(1, 64, ks=3, stride=1, pad=1)\n",
        "        self.branch3x3_final = conv_layer(64, 64, ks=3, stride=1, pad=1)\n",
        "        \n",
        "        self.branch5x5_init = conv_layer(1, 16, ks=5, stride=1, pad=2)\n",
        "        self.branch5x5_final = conv_layer(16, 16, ks=5, stride=1, pad=2)\n",
        "        \n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        x1 = self.branch1x1(x)\n",
        "        \n",
        "        x2 = self.branch1x1_pool(x)\n",
        "        x3 = self.branch1x1_final(x2)\n",
        "        \n",
        "        x4 = self.branch3x3_init(x)\n",
        "        x5 = self.branch3x3_final(x4)\n",
        "        \n",
        "        x6 = self.branch5x5_init(x)\n",
        "        x7 = self.branch5x5_final(x6)\n",
        "        \n",
        "        ans = [x1,x3,x5,x7]\n",
        "        return self.relu(torch.cat(ans,1))\n",
        "\n",
        "class InceptionModule_2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.branch1x1 = conv_layer(128, 64, ks=1, stride=1, pad=0)\n",
        "        \n",
        "        self.branch1x1_pool = conv_layer(128, 32, ks=1, stride=1, pad=0)\n",
        "        self.branch1x1_final = conv_layer(32, 32, ks=1, stride=1, pad=0)\n",
        "        \n",
        "        self.branch3x3_init = conv_layer(128, 128, ks=3, stride=1, pad=1)\n",
        "        self.branch3x3_final = conv_layer(128, 128, ks=3, stride=1, pad=1)\n",
        "        \n",
        "        self.branch5x5_init = conv_layer(128, 32, ks=5, stride=1, pad=2)\n",
        "        self.branch5x5_final = conv_layer(32, 32, ks=5, stride=1, pad=2)\n",
        "        \n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        x1 = self.branch1x1(x)\n",
        "        \n",
        "        x2 = self.branch1x1_pool(x)\n",
        "        x3 = self.branch1x1_final(x2)\n",
        "        \n",
        "        x4 = self.branch3x3_init(x)\n",
        "        x5 = self.branch3x3_final(x4)\n",
        "        \n",
        "        x6 = self.branch5x5_init(x)\n",
        "        x7 = self.branch5x5_final(x6)\n",
        "        \n",
        "        ans = [x1,x3,x5,x7]\n",
        "        return self.relu(torch.cat(ans,1))\n",
        "\n",
        "class InceptionModule_3(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.branch1x1 = conv_layer(256, 128, ks=1, stride=1, pad=0)\n",
        "        \n",
        "        self.branch1x1_pool = conv_layer(256, 64, ks=1, stride=1, pad=0)\n",
        "        self.branch1x1_final = conv_layer(64, 64, ks=1, stride=1, pad=0)\n",
        "        \n",
        "        self.branch3x3_init = conv_layer(256, 256, ks=3, stride=1, pad=1)\n",
        "        self.branch3x3_final = conv_layer(256, 256, ks=3, stride=1, pad=1)\n",
        "        \n",
        "        self.branch5x5_init = conv_layer(256, 64, ks=5, stride=1, pad=2)\n",
        "        self.branch5x5_final = conv_layer(64, 64, ks=5, stride=1, pad=2)\n",
        "        \n",
        "        self.relu = nn.ReLU()\n",
        "        \n",
        "    def forward(self, x):\n",
        "        \n",
        "        x1 = self.branch1x1(x)\n",
        "        \n",
        "        x2 = self.branch1x1_pool(x)\n",
        "        x3 = self.branch1x1_final(x2)\n",
        "        \n",
        "        x4 = self.branch3x3_init(x)\n",
        "        x5 = self.branch3x3_final(x4)\n",
        "        \n",
        "        x6 = self.branch5x5_init(x)\n",
        "        x7 = self.branch5x5_final(x6)\n",
        "        \n",
        "        ans = [x1,x3,x5,x7]\n",
        "        return self.relu(torch.cat(ans,1))\n",
        "\n",
        "def conv_layer_averpl(ni, nf):\n",
        "    aver_pl = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "    return nn.Sequential(conv_layer(ni, nf), aver_pl)\n",
        "\n",
        "model_ft = nn.Sequential(\n",
        "    InceptionModule_1(),\n",
        "    InceptionModule_2(),\n",
        "    InceptionModule_3(),\n",
        "    nn.AdaptiveAvgPool2d((2,2)),\n",
        "    nn.Flatten(),\n",
        "    nn.Linear(2048, 40),\n",
        "    nn.Linear(40, 10)\n",
        ")"
      ],
      "metadata": {
        "id": "qZmFMi9qXZuT"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "lw3u2blpST-F"
      },
      "outputs": [],
      "source": [
        "# model_ft = models.resnet18(pretrained=True) ### Can change this to try out other models available \n",
        "# num_ftrs = model_ft.fc.in_features\n",
        "# Here the size of each output sample is set to 2.\n",
        "# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
        "# model_ft.fc = nn.Linear(num_ftrs, 10)\n",
        "\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1DLjtpyaST-H",
        "outputId": "0f4d6c93-79c6-4143-ad5c-936be2e4563b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/19\n",
            "----------\n",
            "train Loss: 2.0506 Acc: 0.2561\n",
            "val Loss: 1.7607 Acc: 0.4200\n",
            "\n",
            "Epoch 1/19\n",
            "----------\n",
            "train Loss: 1.7101 Acc: 0.3848\n",
            "val Loss: 1.5906 Acc: 0.4333\n",
            "\n",
            "Epoch 2/19\n",
            "----------\n",
            "train Loss: 1.5633 Acc: 0.4320\n",
            "val Loss: 1.5800 Acc: 0.4667\n",
            "\n",
            "Epoch 3/19\n",
            "----------\n",
            "train Loss: 1.4594 Acc: 0.4793\n",
            "val Loss: 1.9752 Acc: 0.3133\n",
            "\n",
            "Epoch 4/19\n",
            "----------\n",
            "train Loss: 1.3774 Acc: 0.5036\n",
            "val Loss: 1.3997 Acc: 0.5400\n",
            "\n",
            "Epoch 5/19\n",
            "----------\n",
            "train Loss: 1.3555 Acc: 0.5236\n",
            "val Loss: 1.3856 Acc: 0.5533\n",
            "\n",
            "Epoch 6/19\n",
            "----------\n",
            "train Loss: 1.2912 Acc: 0.5293\n",
            "val Loss: 1.4866 Acc: 0.5267\n",
            "\n",
            "Epoch 7/19\n",
            "----------\n",
            "train Loss: 1.1257 Acc: 0.6052\n",
            "val Loss: 1.3187 Acc: 0.6000\n",
            "\n",
            "Epoch 8/19\n",
            "----------\n",
            "train Loss: 1.1266 Acc: 0.6123\n",
            "val Loss: 1.3144 Acc: 0.5533\n",
            "\n",
            "Epoch 9/19\n",
            "----------\n",
            "train Loss: 1.0861 Acc: 0.6295\n",
            "val Loss: 1.2741 Acc: 0.5667\n",
            "\n",
            "Epoch 10/19\n",
            "----------\n",
            "train Loss: 1.0496 Acc: 0.6581\n",
            "val Loss: 1.2642 Acc: 0.5867\n",
            "\n",
            "Epoch 11/19\n",
            "----------\n",
            "train Loss: 1.0635 Acc: 0.6538\n",
            "val Loss: 1.3163 Acc: 0.5800\n",
            "\n",
            "Epoch 12/19\n",
            "----------\n",
            "train Loss: 1.0680 Acc: 0.6509\n",
            "val Loss: 1.2641 Acc: 0.5400\n",
            "\n",
            "Epoch 13/19\n",
            "----------\n",
            "train Loss: 1.0221 Acc: 0.6652\n",
            "val Loss: 1.2746 Acc: 0.5933\n",
            "\n",
            "Epoch 14/19\n",
            "----------\n",
            "train Loss: 1.0209 Acc: 0.6795\n",
            "val Loss: 1.2451 Acc: 0.6067\n",
            "\n",
            "Epoch 15/19\n",
            "----------\n",
            "train Loss: 1.0187 Acc: 0.6624\n",
            "val Loss: 1.2463 Acc: 0.5733\n",
            "\n",
            "Epoch 16/19\n",
            "----------\n",
            "train Loss: 1.0304 Acc: 0.6652\n",
            "val Loss: 1.2486 Acc: 0.6000\n",
            "\n",
            "Epoch 17/19\n",
            "----------\n",
            "train Loss: 1.0318 Acc: 0.6481\n",
            "val Loss: 1.2469 Acc: 0.5933\n",
            "\n",
            "Epoch 18/19\n",
            "----------\n",
            "train Loss: 1.0225 Acc: 0.6595\n",
            "val Loss: 1.2486 Acc: 0.5933\n",
            "\n",
            "Epoch 19/19\n",
            "----------\n",
            "train Loss: 1.0185 Acc: 0.6781\n",
            "val Loss: 1.2467 Acc: 0.6067\n",
            "\n",
            "Training complete in 15m 23s\n",
            "Best val Acc: 0.606667\n"
          ]
        }
      ],
      "source": [
        "# 1 inception module\n",
        "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
        "                           num_epochs=20)\n",
        "# torch.save(model_ft.state_dict(),\"weights.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "WSa2LdN6ST-J"
      },
      "outputs": [],
      "source": [
        "torch.save(model_ft.state_dict(),\"/content/drive/MyDrive/one_inception_weights.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 3 inception module with skip connections\n",
        "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
        "                           num_epochs=20)\n",
        "# torch.save(model_ft.state_dict(),\"weights.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qO_ySkyHZpSe",
        "outputId": "b0057743-e13a-45fe-d884-863a55911e90"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/19\n",
            "----------\n",
            "train Loss: 2.1270 Acc: 0.2117\n",
            "val Loss: 1.8364 Acc: 0.3600\n",
            "\n",
            "Epoch 1/19\n",
            "----------\n",
            "train Loss: 1.7387 Acc: 0.3591\n",
            "val Loss: 2.2074 Acc: 0.2867\n",
            "\n",
            "Epoch 2/19\n",
            "----------\n",
            "train Loss: 1.6429 Acc: 0.3991\n",
            "val Loss: 1.8803 Acc: 0.4000\n",
            "\n",
            "Epoch 3/19\n",
            "----------\n",
            "train Loss: 1.6193 Acc: 0.4034\n",
            "val Loss: 1.6666 Acc: 0.4400\n",
            "\n",
            "Epoch 4/19\n",
            "----------\n",
            "train Loss: 1.4760 Acc: 0.4592\n",
            "val Loss: 2.0453 Acc: 0.3333\n",
            "\n",
            "Epoch 5/19\n",
            "----------\n",
            "train Loss: 1.4376 Acc: 0.4764\n",
            "val Loss: 1.8185 Acc: 0.3533\n",
            "\n",
            "Epoch 6/19\n",
            "----------\n",
            "train Loss: 1.3874 Acc: 0.5050\n",
            "val Loss: 1.5987 Acc: 0.4800\n",
            "\n",
            "Epoch 7/19\n",
            "----------\n",
            "train Loss: 1.2134 Acc: 0.5508\n",
            "val Loss: 1.3199 Acc: 0.5667\n",
            "\n",
            "Epoch 8/19\n",
            "----------\n",
            "train Loss: 1.1750 Acc: 0.5851\n",
            "val Loss: 1.2979 Acc: 0.5867\n",
            "\n",
            "Epoch 9/19\n",
            "----------\n",
            "train Loss: 1.1452 Acc: 0.5994\n",
            "val Loss: 1.3074 Acc: 0.5867\n",
            "\n",
            "Epoch 10/19\n",
            "----------\n",
            "train Loss: 1.1062 Acc: 0.6037\n",
            "val Loss: 1.3447 Acc: 0.6000\n",
            "\n",
            "Epoch 11/19\n",
            "----------\n",
            "train Loss: 1.1423 Acc: 0.5980\n",
            "val Loss: 1.3106 Acc: 0.5800\n",
            "\n",
            "Epoch 12/19\n",
            "----------\n",
            "train Loss: 1.1344 Acc: 0.5851\n",
            "val Loss: 1.3164 Acc: 0.6067\n",
            "\n",
            "Epoch 13/19\n",
            "----------\n",
            "train Loss: 1.1198 Acc: 0.6166\n",
            "val Loss: 1.3095 Acc: 0.6067\n",
            "\n",
            "Epoch 14/19\n",
            "----------\n",
            "train Loss: 1.1074 Acc: 0.6066\n",
            "val Loss: 1.2949 Acc: 0.6067\n",
            "\n",
            "Epoch 15/19\n",
            "----------\n",
            "train Loss: 1.0621 Acc: 0.6352\n",
            "val Loss: 1.2785 Acc: 0.6133\n",
            "\n",
            "Epoch 16/19\n",
            "----------\n",
            "train Loss: 1.0845 Acc: 0.6195\n",
            "val Loss: 1.2825 Acc: 0.6200\n",
            "\n",
            "Epoch 17/19\n",
            "----------\n",
            "train Loss: 1.0809 Acc: 0.6094\n",
            "val Loss: 1.2833 Acc: 0.6067\n",
            "\n",
            "Epoch 18/19\n",
            "----------\n",
            "train Loss: 1.0812 Acc: 0.6280\n",
            "val Loss: 1.2668 Acc: 0.5800\n",
            "\n",
            "Epoch 19/19\n",
            "----------\n",
            "train Loss: 1.1025 Acc: 0.6166\n",
            "val Loss: 1.2818 Acc: 0.5933\n",
            "\n",
            "Training complete in 19m 29s\n",
            "Best val Acc: 0.620000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model_ft.state_dict(),\"/content/drive/MyDrive/inception_with_skip_weights.pth\")"
      ],
      "metadata": {
        "id": "FgRhFO-dZsVh"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model_ft = models.resnet18(pretrained=True) ### Can change this to try out other models available \n",
        "# num_ftrs = model_ft.fc.in_features\n",
        "# Here the size of each output sample is set to 2.\n",
        "# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names)).\n",
        "# model_ft.fc = nn.Linear(num_ftrs, 10)\n",
        "\n",
        "model_ft = model_ft.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
      ],
      "metadata": {
        "id": "DOWQA70ViDlh"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3 inception module with skip connections\n",
        "model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n",
        "                           num_epochs=20)\n",
        "# torch.save(model_ft.state_dict(),\"weights.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 415
        },
        "id": "QTvTlbeXiGle",
        "outputId": "c1348f18-c371-4af0-ce0e-75d7a90ee89a"
      },
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0/19\n",
            "----------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-89-26f29c68f792>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 3 inception module with skip connections\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m model_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n\u001b[0;32m----> 3\u001b[0;31m                            num_epochs=20)\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# torch.save(model_ft.state_dict(),\"weights.pth\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-51-6d8ba338b924>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0;31m# track history if only in train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m                     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-87-7944407e5f0f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mans\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mInceptionModule_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 294.00 MiB (GPU 0; 11.17 GiB total capacity; 10.17 GiB already allocated; 14.81 MiB free; 10.62 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model_ft.state_dict(),\"/content/drive/MyDrive/3_cons_inception_weights.pth\")"
      ],
      "metadata": {
        "id": "Fn69tbcciJVY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2bqzw9fST-N"
      },
      "outputs": [],
      "source": [
        "model_ft.load_state_dict(torch.load(\"weights.pth\"))\n",
        "model_ft = model_ft.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F1WeIhMrST-P"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Observe that all parameters are being optimized\n",
        "optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Decay LR by a factor of 0.1 every 7 epochs\n",
        "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mGofdXp3ST-Q"
      },
      "outputs": [],
      "source": [
        "def test_model(model, criterion, optimizer, scheduler):\n",
        "\n",
        "    model.eval()   # Set model to evaluate mode\n",
        "\n",
        "    t_output = []\n",
        "    t_pred = []\n",
        "    y_test = []\n",
        "    top_k = []\n",
        "    # Iterate over data.\n",
        "    i = 1\n",
        "    for inputs, labels in dataloaders['test']:\n",
        "        \n",
        "        inputs = inputs.to(device)\n",
        "        labels = labels.to(device)\n",
        "        y_test.append(labels)\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward\n",
        "        # track history if only in train\n",
        "        with torch.set_grad_enabled(True):\n",
        "            outputs = model(inputs.float())\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            loss = criterion(outputs, labels)\n",
        "            t_output.append(outputs)\n",
        "            t_pred.append(preds)\n",
        "            temp1, temp2 = outputs.topk(5)\n",
        "            top_k.append(temp2)\n",
        "\n",
        "    y_test = torch.cat(y_test).cpu().detach().numpy() \n",
        "    y_test_num = torch.cat(t_pred).cpu().detach().numpy() \n",
        "    y_pred = torch.cat(top_k).cpu().detach().numpy() \n",
        "    print('\\nConfusion Matrix')\n",
        "    conf_mt = confusion_matrix(y_test_num, y_test)\n",
        "    print(conf_mt)\n",
        "    plt.matshow(conf_mt)\n",
        "    plt.show()\n",
        "    print('\\nClassification Report')\n",
        "    print(classification_report(y_test_num, y_test))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q2-v3_n0ST-R",
        "outputId": "4f38f58d-b2ca-434d-85fb-f2e62673d052"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "CUDA out of memory. Tried to allocate 74.00 MiB (GPU 0; 8.00 GiB total capacity; 6.34 GiB already allocated; 52.79 MiB free; 6.46 GiB reserved in total by PyTorch)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-cf44ecd268a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer_ft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexp_lr_scheduler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-30-5e5331cb87ff>\u001b[0m in \u001b[0;36mtest_model\u001b[0;34m(model, criterion, optimizer, scheduler)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# track history if only in train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2d_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mconv2d_forward\u001b[0;34m(self, input, weight)\u001b[0m\n\u001b[1;32m    340\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    341\u001b[0m         return F.conv2d(input, weight, self.bias, self.stride,\n\u001b[0;32m--> 342\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 74.00 MiB (GPU 0; 8.00 GiB total capacity; 6.34 GiB already allocated; 52.79 MiB free; 6.46 GiB reserved in total by PyTorch)"
          ]
        }
      ],
      "source": [
        "test_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U82tX-8GST-Z"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "deep_learning_base_code_inception_module.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}